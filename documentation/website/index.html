<!doctype html><html class="no-js"><head><meta charset="utf-8"><title>Project Title</title><meta name="description" content=""><meta name="viewport" content="width=device-width">
<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
<link href="http://fonts.googleapis.com/css?family=Raleway:300,400,600" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="style.css">
        <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <!--<link rel="stylesheet" href="styles/main.37ab405b.css">-->
<body>
<script defer src="scripts/load_model_data.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" crossorigin="anonymous"></script>
<script defer src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js" crossorigin="anonymous"></script>

<!--[if lt IE 7]>
<p class="browsehappy">You are using an 
    <strong>outdated</strong> browser. Please 
    <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.
</p>
<![endif]-->
<div class="container">

    <nav class="navbar">
        <div class="container">
            <ul class="navbar-list">
                <li class="navbar-item">
                    <a class="navbar-link" href="#intro">Abstract</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#intro">Intro</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#method">Method</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#experiments">Experiments</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#discussion">Discussion</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#appendix">Appendix</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#references">References</a>
                </li>
            </ul>
        </div>
    </nav>

    <section class="header" id="abstract">
        <h2 class="title">ABC dataset segmentation with MeshCNN</h2>
        <h6>Project by Andr√©s Mandado (<a href="mailto:mandadoalmajano@campus.tu-berlin.de">mandadoalmajano@campus.tu-berlin.de"</a>)
        </h6>

        <div class="row">
<!--            <div class="one column category" style="text-align: center;">-->
<!--                <h5 class="docs-header">Teaser Image</h5>-->
                <img class="u-max-full-width" src="images/teaser.jpg">
                    <p>Abstract</p>
        </div>
    </section>

    <div class="docs-section" id="intro">
        <h3 class="section-heading">Introduction</h3>
        <h4 class="subsection-heading">MeshCNN</h4>
        <p>
            MeshCNN has different architectures for classification and segmentation. For segmentation,
            it uses a UNet architecture with residual learning blocks.
        </p>
        <figure>
            <img class="u-max-full-width" src="images/MeshCNN.svg">
            <figcaption>MeshCNN ResUNet</figcaption>
        </figure>
        <div>
            <p>MeshCNN uses mesh edges as primitives. For each each, it computes 5 input features:
            </p>
            <ul>
                <li>Dihedral angle between two incident faces</li>
                <li>2 opposite vertex angles</li>
                <li>2 ratios of edge length wrt triangle height</li>
            </ul>
            <div class="u-pull-right" style="width:20%">
                <figure>
                    <img class="u-max-full-width" src="images/input_features.svg">
                    <figcaption>Input Edge Features</figcaption>
                </figure>
                <figure>
                    <img class="u-max-full-width" src="images/mesh_conv.png">
                    <figcaption>Mesh Convolution</figcaption>
                </figure>
            </div>
            <img style="display:block;margin:auto" width="50%" src="images/input_vector.svg">
            <p>All these features are relative and implicitly invariant to translation, rotation and unform scaling.
            </p>
        </div>
        <p>Convolution is applied to the 1-ring neighbourhood of each edge, made of 5 edges (4 edges and itself). Edge
            order invariance is ensure by applying symmetric functions:
        </p>
        <img style="display:block;margin:auto" width="50%" src="images/neighbourhood.svg">
        <div style="clear:both;display: block"/>
         <p>In the pooling layer, a mesh is simplified to a target number of edges (model parameter) collapsing sequentially
             those edges with the smallest feature norm. A collapsed edge is removed and the features of the neighbouring
             edges are averaged by pairs. The correspondence between new and old edges is saved and used during unpooling to
             recover the original mesh structure.
        </p>
        <div style="width:50%; margin:auto">
            <figure>
                <img height="100%" width="100%" src="images/mesh_pool_unpool.png">
                <figcaption>Mesh Pooling/Unpooling</figcaption>
            </figure>
        </div>
        <h4 class="subsection-heading">ABC Dataset</h4>
        <div class="u-pull-right" style="width:30%">
            <figure>
                <img class="u-max-full-width" src="images/abcmodel.png">
                <figcaption>ABC dataset model</figcaption>
            </figure>
        </div>
        <ul>
            <li>1M CAD models</li>
            <li>Explicitely parameterized surfaces/curves</li>
            <li>High-quality triangle meshes</li>
        </ul>
        <div style="clear:both;display: block"/>
    </div>

    <div class="docs-section" id="method">
        <h3 class="section-heading">Method</h3>
        <p class="section-description"></p>
        <h4 class="subsection-heading">MeshCNN improvements</h4>
        <p>Initial experiments showed that MeshCNN is not optimized to handle large meshes. The experiments in the
           original paper use datasets with small meshes, having at most a few thousand edges.<br><br>
           The first bottleneck is GPU memory usage, which grows quadratically with the number of mesh edges. Each pooling
            layer keeps in memory a tensor of \(edges^{2}\) elements mapping edge collapses (<a href="https://github.com/ranahanocka/MeshCNN/blob/15b83cc6a4db968baf6cf595df78995a9c2dcee3/models/layers/mesh_union.py#L5"><i>MeshUnion</i></a> class).
            This makes impossible training with half of the meshes in the ABC dataset (a single pooling operation in a 50K-edge mesh would allocate ~18GB of memory just for edge collapse bookkeeping!).<br>
            \[
            U = \begin{bmatrix}
                u_{11} & u_{12} & \dots \\
                \vdots & \ddots & \\
                u_{N1} &        & u_{NM}
                \end{bmatrix} \begin{cases}
              M = \text{#input edges}\\
              N = \text{#output edges}\\
              u_{ij} = \text{# collapses of input edge i into output edge j}
            \end{cases}
            \],<br>
            The tensor though is inherently sparse, therefore we reimplemented <i>MeshUnion</i> and <a href="https://github.com/ranahanocka/MeshCNN/blob/15b83cc6a4db968baf6cf595df78995a9c2dcee3/models/layers/mesh_unpool.py#L6"><i>MeshUnpool</i></a> classes using <a href="https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html"><i>torch.sparse_coo_tensor</i></a>
            and sparse operations such as  <a href="https://pytorch.org/docs/stable/sparse.html#torch.sparse.mm"><i>torch.sparse.mm</i></a> and <a href="https://pytorch.org/docs/stable/sparse.html#torch.sparse.mm"><i>torch.sparse.sum</i></a>.
            The memory consumption at function <a href="https://github.com/ranahanocka/MeshCNN/blob/15b83cc6a4db968baf6cf595df78995a9c2dcee3/models/layers/mesh_conv.py#L39"><i>create_GeMM</i></a> in the <a href="https://github.com/ranahanocka/MeshCNN/blob/15b83cc6a4db968baf6cf595df78995a9c2dcee3/models/layers/mesh_conv.py#L5"><i>MeshConv</i></a>
            layer was also reduced to less than a half by deleting temporary tensors once they're not needed and rewriting the symmetric functions so that they are done in place.<br><br>
            Another important bottleneck is that the edge collapsing at the MeshPool layer is done sequentially in CPU. As a result, MeshCNN training is
            CPU-bound and GPU utilization is low (under 30% in a NVIDIA Tesla P100). Training times become impractical for small datasets
            (9h per epoch for 1K meshes of 35K edges). To alleviate this problem, we modified MeshCNN code to allow for distributed training across multiple CPUs, GPUs and
            even nodes. Using <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html"><i>torch.nn.parallel.DistributedDataParallel</i></a>, each batch
            is split and processed by identical instances of the model in separate devices, averaging the gradients for each node during the backwards pass.<br><br>
            Thanks to these optimizations we could train MeshCNN with ABC models of average size (~35K edges) and typical batch sizes (~10), having still room to increase
            the number of pooling layers.<br>
        </p>
        <h4 class="subsection-heading">Dataset preprocessing</h4>
        <p>As first preprocessing step for the compilation of dataset, we detect and exclude non-manifold meshes, since they're not supported by MeshCNN.<br>
           Additionally, small connected components (<10% of total faces) within a model are likely to become non-manifold during pooling. Models with such small
           components had to be excluded with the original MeshCNN code. To overcome this problem, we've modified MeshCNN to skip edge collapses that would result
            in a non-manifold mesh.
        </p>
           <div class="row">
                <div class="one-half column category">
                    <h5 style="text-align:center">Non-manifold mesh (Model 34471)</h5>
                    <figure>
                        <img height="100%" width="100%" src="images/non_manifold_mesh_34471.png">
                    </figure>
                </div>
                <div class="one-half column category">
                    <h5 style="text-align:center">Small component (Model 49466)</h5>
                    <script class="model" src="models/49466_small_component.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
                    {"version_major": 2, "version_minor": 0, "model_id": "94b8f1e32a54489c881eac3b91ce4dcc"}
                    </script>
                </div>
            </div>
        <h4 class="subsection-heading">Dataset remeshing</h4>
        <p>ABC models are fairly large, some of them made of thousands of surface patches. The dataset comes with high resolution meshes, going from a few thousand edges to
            hundreds of thousands. MeshCNN UNet architecture does class segmentation by applying a succession of convolution and pooling operations to the input features. Logically,
            the larger the input meshes are, the more conv+pool stages are needed in the architecture.
            The PAC framework estimates a minimum training sample size of O((VC + log(1/eta))/eps) to ensure the test error is not more than eps w.r.t the
            training error with a confidence of (1-eta). The VC-dimension of a binary NN classifier is O(|w|), w being the #weights. MeshCNN has ~2.2M weights in the original paper configuration
            (4 conv layers of 32,64,128,256 channels). The #weights increases proportionately with the number of conv channels and so does therefore the theorical min. training size.
            Given the performance bottleneck cause by in-CPU edge collapsing, training average ABC mesh sizes (~35K) in MeshCNN becomes impractical.<br>
            We've therefore remeshed ABC models using cadmesh to get a target size of 2000 edges. Since almost all ABC original meshes are larger than that, the error of the mesh w.r.t to the model
            original surfaces had to be limited (using gmsh's element size setting MeshSizeFromCurvature). The remesh was attempted for the first 100K ABC models, from which only ~10K could be remeshed
            into 2K meshes within the error bounds. Since gmsh cannot control the exact size of the output mesh, a 2% tolerance in the final #edges was applied, accepting only meshes with #edges in range
            (1960-2000). Meshes smaller than target size are padded with invalid values during training, ignoring their contribution during backward propagation.
        </p>
        <h4 class="subsection-heading">Dataset augmentation</h4>
        <div class="u-pull-right" style="width:50%; margin-left:20px">
            <figure>
                <img class="u-max-full-width" src="images/class_imbalance_chart.svg">
                <figcaption>Edge class frequencies on 35K-edge mesh dataset</figcaption>
            </figure>
        </div>
        <p>Surfaces in ABC dataset models are predominantly planes and cylinders, with less than 10% of the surfaces belonging to other classes. This imbalance in the
            surface class occurrence biases the model towards the overrepresented classes and impedes the learning of the underrepresented surface types.Experiments applying
            a weighted loss function to penalize more misclasiffications of the underrepresented classes show no noticeable improvement.<br>
            The class imbalance was addressed by resampling. Any model made of exclusively of planes and cylinders was discarded, hence downsampling these classes. Additionally, synthetic samples were
            created for the other classes to upsample them. The synthetic samples were created using gmsh to mesh parametric surfaces with uniformly distributed random parameters.<br>
            The first synthetic samples were open meshes made of a single surface, thus having a fine control in the class upsampling. This approach has two problems: <br>
            1. MeshCNN applies boundary-preserving edge collapses during pooling. As a result, the prediction accuracy near the mesh boundary is severely affected<br>
            2. The model cannot learn to identify class boundaries with Synthetic samples made of a single surface.<br>
            Thefore, the final synthetic dataset was composed of closed meshes made of multiple surface types.
        </p>
        <div class="row">
                <div class="one-half column category">
                    <h5 style="text-align:center">Cone open mesh (original size)</h5>

                    <script class="model" src="models/open_cone_0.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
                        {"version_major": 2, "version_minor": 0, "model_id": "df851097a6754f5c99f2bb48bb52c530"}
                    </script>
                </div>
                <div class="one-half column category">
                    <h5 style="text-align:center">Cone open mesh (after last pooling)</h5>
                    <script class="model" src="models/open_cone_3.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
                        {"version_major": 2, "version_minor": 0, "model_id": "22c489139c4e4b709192a78bab5b311b"}
                    </script>
                </div>
                <table class="u-full-width">
                    <tbody class="surface-types-table">
                        <tr>
                            <td class="surface-types" style="background-color:red">Plane</td>
                            <td class="surface-types" style="background-color:blue">Cylinder</td>
                            <td class="surface-types" style="background-color:magenta">Cone</td>
                            <td class="surface-types" style="background-color:purple">Torus</td>
                            <td class="surface-types" style="background-color:black">BSpline</td>
                            <td class="surface-types" style="background-color:orange">Sphere</td>
                            <td class="surface-types" style="background-color:green">Revolution</td>
                            <td class="surface-types" style="background-color:cyan">Extrusion</td>
                            <td class="surface-types" style="background-color:yellow">Other</td>
                        </tr>
                    </tbody>
                </table>
            </div>
    </div>
    <div style="clear:both;display: block"/>

    <div class="docs-section" id="experiments">
        <h3 class="section-heading">Experiments</h3>
        <p class="section-description"></p>
        <div>
            <h4 class="subsection-heading">Ice breaker</h4>
            <div>
                <div class="u-pull-right-third">
                    <table class="u-full-width">
                            <tr>
                                <th>Train set size</th>
                                <td>262</td>
                            </tr>
                            <tr>
                                <th>Test set size</th>
                                <td>30</td>
                            </tr>
                            <tr>
                                <th>Mesh size</th>
                                <td><=5000 edges</td>
                            </tr>
                            <tr>
                                <th>Convolution #channels</th>
                                <td>32, 64, 128, 256</td>
                            </tr>
                            <tr>
                                <th>Pooling target sizes</th>
                                <td>4000, 3000, 1800</td>
                            </tr>
                            <tr>
                                <th>Batch size</th>
                                <td>4</td>
                            </tr>
                            <tr>
                                <th>Test accuracy</th>
                                <td>91.6% (15 epochs)</td>
                            </tr>
                    </table>
                </div>
                <p>In this first experiment we trained MeshCNN with a toy ABC subset of size and mesh resolutions similar to those
                    used in <a href="#1">[1]</a>, with the following objectives:
                </p>
                <ul>
                    <li>Validate training setup, label generation in particular</li>
                    <li>Early feasibility check</li>
                    <li>Identify potential changes required in MeshCNN and dataset preprocessing</li>
                </ul>
                <p>As a result of the experiment, we identified the memory and processing bottlenecks as well as required preprocessing steps
                explained in the <a href="#method">methodology</a>.</p>.
            </div>
            <div style="clear:both;display: block"/>
            <div>
                <h5 style="text-align:center">Model 36631 (prediction)</h5>
                <script class="model" src="models/36631.json" type="application/vnd.jupyter.widget-state+json"></script>
                <script type="application/vnd.jupyter.widget-view+json">
                {"version_major": 2, "version_minor": 0, "model_id": "6c90cad6836149d7a89e244e20cc9dde"}
            </script>
                <table width="100%" style="table-layout: fixed">
                    <tbody class="surface-types-table">
                        <tr>
                            <td class="surface-types" style="background-color:red">Plane</td>
                            <td class="surface-types" style="background-color:blue">Cylinder</td>
                            <td class="surface-types" style="background-color:magenta">Cone</td>
                            <td class="surface-types" style="background-color:purple">Torus</td>
                            <td class="surface-types" style="background-color:black">BSpline</td>
                            <td class="surface-types" style="background-color:orange">Sphere</td>
                            <td class="surface-types" style="background-color:green">Revolution</td>
                            <td class="surface-types" style="background-color:cyan">Extrusion</td>
                            <td class="surface-types" style="background-color:yellow">Other</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <div>
            <h4 class="subsection-heading">35K-edge mesh dataset</h4>
            <div class="u-pull-right-third">
                    <table class="u-full-width">
                            <tr>
                                <th>Train set size</th>
                                <td>862</td>
                            </tr>
                            <tr>
                                <th>Test set size</th>
                                <td>93</td>
                            </tr>
                            <tr>
                                <th>Mesh size</th>
                                <td>[33000,35000] edges</td>
                            </tr>
                            <tr>
                                <th>Convolution #channels</th>
                                <td>32, 64, 128, 256</td>
                            </tr>
                            <tr>
                                <th>Pooling target sizes</th>
                                <td>20000, 15000, 10000</td>
                            </tr>
                            <tr>
                                <th>Batch size</th>
                                <td>10</td>
                            </tr>
                            <tr>
                                <th>Test accuracy</th>
                                <td>89.5% (19 epochs)</td>
                            </tr>
                    </table>
            </div>
            <p>Onced the issues observed in the first experiment were solved a second larger dataset was compiled with meshes of
                sizes close to the ABC average. The meshes in the dataset have size differences (in #edges) of at most 10%, since
                mixing mesh of very different sizes may have an impact on the network performance (as is the case with CNNs for images).
            </p>
            <div style="clear:both;display: block"/>
            <div class="row">
                <div class="one-half column category">
                    <h5 style="text-align:center">Model 38019 (prediction)</h5>
                    <script class="model" src="models/38019_pred.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
    {"version_major": 2, "version_minor": 0, "model_id": "6579d789b1af4d3c8a003277cd08db58"}
    </script>
                </div>
                <div class="one-half column category">
                    <h5 style="text-align:center">Model 38019 (ground truth)</h5>
                    <script class="model" src="models/38019_truth.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
    {"version_major": 2, "version_minor": 0, "model_id": "c890054592ea4b1886731e2997ca026e"}
    </script>
                </div>
                <table class="u-full-width">
                    <tbody class="surface-types-table">
                        <tr>
                            <td class="surface-types" style="background-color:red">Plane</td>
                            <td class="surface-types" style="background-color:blue">Cylinder</td>
                            <td class="surface-types" style="background-color:magenta">Cone</td>
                            <td class="surface-types" style="background-color:purple">Torus</td>
                            <td class="surface-types" style="background-color:black">BSpline</td>
                            <td class="surface-types" style="background-color:orange">Sphere</td>
                            <td class="surface-types" style="background-color:green">Revolution</td>
                            <td class="surface-types" style="background-color:cyan">Extrusion</td>
                            <td class="surface-types" style="background-color:yellow">Other</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="row">
                <div class="one-half column category">
                    <h5 style="text-align:center">Model 38191 (prediction)</h5>
                     <script class="model" src="models/38191_pred.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
    {"version_major": 2, "version_minor": 0, "model_id": "5c7ac1c66a87465380401a42895e09d3"}
    </script>
                </div>
                <div class="one-half column category">
                    <h5 style="text-align:center">Model 38191 (ground truth)</h5>
                    <script class="model" src="models/38191_truth.json" type="application/vnd.jupyter.widget-state+json"></script>
                    <script type="application/vnd.jupyter.widget-view+json">
    {"version_major": 2, "version_minor": 0, "model_id": "73e2c1b19d954ba694b96d22fd9646dc"}
    </script>
                </div>
                <table class="u-full-width">
                    <tbody class="surface-types-table">
                        <tr>
                            <td class="surface-types" style="background-color:red">Plane</td>
                            <td class="surface-types" style="background-color:blue">Cylinder</td>
                            <td class="surface-types" style="background-color:magenta">Cone</td>
                            <td class="surface-types" style="background-color:purple">Torus</td>
                            <td class="surface-types" style="background-color:black">BSpline</td>
                            <td class="surface-types" style="background-color:orange">Sphere</td>
                            <td class="surface-types" style="background-color:green">Revolution</td>
                            <td class="surface-types" style="background-color:cyan">Extrusion</td>
                            <td class="surface-types" style="background-color:yellow">Other</td>
                        </tr>
                    </tbody>
            </table>
            </div>
             <p> The model learns to predict the two most frequent classes (plane and cylinder surfaces), but no the rest. The optimization seems to stay at a local minima in the loss
                function, most probably due to the imbalance in the class occurrence.
            </p>
        </div>
        <div style="clear:both;display: block"/>
        <h4 class="subsection-heading">Medium dataset (weighted)</h4>
        <p>To escape the local minima reached in the previous experiment, we apply weights to the loss function (torch.nn.CrossEntropyLoss) inversely proportional to class occurrence in train set.</p>
        <h4 class="subsection-heading">Medium dataset (resampled & weighted)</h4>
        <p>This experiment uses a new dataset compiled oversampling the underrepresented classes and undersampling the most frequent ones. The remaining class imbalance is compensated by loss
        function adjustments as in the previous experiment.</p>
    </div>

    <div class="docs-section" id="discussion">
        <h3 class="section-heading">Discussion and Future Work</h3>
        <p class="section-description"></p>
    </div>

    <div class="docs-section" id="appendix">
        <h3 class="section-heading">Appendix</h3>
        <ul>
            <li>Presentations
              <ol>
                <li>Project proposal</li>
                <li>Tech presentation</li>
              </ol>
            </li>
        </ul>
    </div>

    <div class="docs-section" id="references">
        <h3 class="section-heading">References</h3>
        <ul class="popover-list">
            <li class="popover-item" id="1">
                <!--A link to this may look like: <a href="#1">[1]</a>-->
                [1] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman and Daniel Cohen-Or. 2019. MeshCNN: A Network with an Edge. <i>ACM Trans. Graph. 1, 1 (Feb 2019). </i><a href="https://ranahanocka.github.io/MeshCNN"> https://ranahanocka.github.io/MeshCNN</a>
            </li>
          
            <li class="popover-item" id="2">
                [2] Koch, Sebastian and Matveev, Albert and Jiang, Zhongshi and Williams, Francis and Artemov, Alexey and Burnaev, Evgeny and Alexa, Marc and Zorin, Denis and Panozzo, Daniele. 2019. ABC: A Big CAD Model Dataset For Geometric Deep Learning. <i>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019). </i><a href="https://deep-geometry.github.io/abc-dataset"> https://deep-geometry.github.io/abc-dataset</a>
            </li>
            
        </ul>
    </div>

</div>

